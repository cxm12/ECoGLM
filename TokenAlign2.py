import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer # AutoModelForCausalLM,
import os
from VQTokenizer2 import NeuralVQVAE
# from torch.utils.data import DataLoader, random_split, Dataset
from data import MillerFingersDataset, BCIComp4Dataset4_single, prepare_dataloaders #  ClassBalancedBatchSampler, random,
import math
from torch.optim.lr_scheduler import LambdaLR
from collections import Counter
import shutil
import numpy as np


# --- 1. æ ‡ç­¾æ˜ å°„é…ç½® ---
# å®šä¹‰ fingerflex å­é›†çš„ label åˆ° æ–‡æœ¬çš„æ˜ å°„
fingerflexLABEL_MAP = {
    0: "Inter-stimulus interval", # -> GT Token ID: 3306
    1: "thumb", # -> GT Token ID: 25036
    2: "index finger", #  -> GT Token ID: 1252
    3: "middle finger", #  -> GT Token ID: 19656
    4: "ring finger", #  -> GT Token ID: 12640
    5: "little finger" # -> GT Token ID: 55392
}
BCILABEL_MAP = fingerflexLABEL_MAP
Gesture_MAP = fingerflexLABEL_MAP
Motor_MAP = {
    0: "blank screen",  # Global ID 10189 (blank screen)
    11: "tongue movement", # Global ID 83 (tongue movement)
    12: "hand movement" # Global ID 10661 (hand movement)
}
LABEL_MAP = fingerflexLABEL_MAP  # Motor_MAP # BCILABEL_MAP  # 

fingerflextoken2_FullToken = {
    3306:[3306, 5477, 318, 19425, 9873],
    25036:[25036],
    1252:[1252, 14317],
    19656:[19656, 14317],
    12640:[12640, 14317],
    55392:[55392, 14317]   
}
GT_token2_FullToken = fingerflextoken2_FullToken


# --- 2. å¢å¼ºå‹æ¨¡å‹ç»„ä»¶ ---
class MultiScaleFeatureExtractor(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        # ä½¿ç”¨ä¸åŒçš„ç©ºæ´ç‡ (dilation) æˆ– æ ¸å¤§å°ï¼Œæ•æ‰å¤šå°ºåº¦ç‰¹å¾
        self.conv1 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=5, padding=2)
        self.conv3 = nn.Conv1d(embed_dim, embed_dim, kernel_size=7, padding=3)
        self.bn = nn.BatchNorm1d(embed_dim * 3)
        self.proj = nn.Linear(embed_dim * 3, embed_dim)

    def forward(self, x):
        # x: [B, T, E] -> [B, E, T]
        x = x.transpose(1, 2)
        c1 = F.gelu(self.conv1(x))
        c2 = F.gelu(self.conv2(x))
        c3 = F.gelu(self.conv3(x))
        out = torch.cat([c1, c2, c3], dim=1)
        out = self.bn(out).transpose(1, 2)
        return self.proj(out)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=1024):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class TransformerVQToTokenModel6_Robust(nn.Module):
    def __init__(self, codebook_size, embed_dim=256, nhead=8, num_layers=3, num_classes=6, max_seq_len=64):
        super().__init__()
        self.embedding = nn.Embedding(codebook_size, embed_dim)
        
        # å¼•å…¥å¤šå°ºåº¦å·ç§¯å¢å¼º
        self.ms_extractor = MultiScaleFeatureExtractor(embed_dim)
        self.pos_encoder = PositionalEncoding(embed_dim, max_seq_len)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, 
            nhead=nhead, 
            dim_feedforward=embed_dim * 4,
            dropout=0.4, # å¢åŠ  Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
            batch_first=True,
            activation='gelu'
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.ln_final = nn.LayerNorm(embed_dim)
        self.fc_head = nn.Sequential(
            nn.Flatten(),
            nn.Linear(max_seq_len * embed_dim, 512),
            nn.BatchNorm1d(512),
            nn.GELU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, vq_indices):
        # [B, T] -> [B, T, E]
        x = self.embedding(vq_indices)
        
        # 1. å·ç§¯æå–å±€éƒ¨ç‰¹å¾
        x = self.ms_extractor(x)
        # 2. ä½ç½®ç¼–ç 
        x = self.pos_encoder(x)
        # 3. Transformer å…¨å±€å»ºæ¨¡
        x = self.transformer_encoder(x)
        # 4. å½’ä¸€åŒ–ä¸åˆ†ç±»
        x = self.ln_final(x)
        logits = self.fc_head(x)
        return logits

# --- 3. è®­ç»ƒå¯¹é½å™¨ç±» ---

class TokenAlignmentTrainer:
    def __init__(self, model_path="../Qwen2.5-7B-Instruct", subset='fingerflex'):
        # ä»…åœ¨éœ€è¦ Token æ˜ å°„æ—¶åˆå§‹åŒ– Tokenizer
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        except:
            self.tokenizer = None
        self.gt_token_map = {}
        if self.tokenizer:
            for idx, text in LABEL_MAP.items():
                tokens = self.tokenizer.encode(text, add_special_tokens=False)
                self.gt_token_map[idx] = tokens[0]

    def get_local_labels(self, labels_tensor):
        return labels_tensor.to(torch.long)


def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5):
    """
    åˆ›å»ºå¸¦é¢„çƒ­çš„ä½™å¼¦è°ƒåº¦å™¨
    num_warmup_steps: é¢„çƒ­çš„ Epoch æ•°
    num_training_steps: æ€»è®­ç»ƒ Epoch æ•°
    """
    def lr_lambda(current_step):
        # 1. çº¿æ€§é¢„çƒ­é˜¶æ®µ
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        
        # 2. ä½™å¼¦é€€ç«é˜¶æ®µ
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))

    return LambdaLR(optimizer, lr_lambda)


from torch.optim.lr_scheduler import ReduceLROnPlateau


# --- æƒé‡å¹³æ»‘ç±» ---
class EMA():
    def __init__(self, model, decay):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}

    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()

    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]

    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}


# --- 2. è®­ç»ƒæµç¨‹ä¼˜åŒ– ---
def train_alignment():
    # ğŸŒŸ ä¼˜åŒ– A: åŠ¨æ€æƒé‡ç­–ç•¥
    # ç»™ç±»0(é™æ¯æ€)æä½çš„æƒé‡ï¼Œç»™å…¶ä»–æ‰‹æŒ‡æ›´é«˜çš„æƒé‡ï¼Œå¼ºåˆ¶æ¨¡å‹è·³å‡ºèˆ’é€‚åŒº
    class_weights = torch.tensor([0.05, 1.2, 1.2, 1.2, 1.2, 1.2]).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)

    optimizer = torch.optim.AdamW(token_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-2)
    
    # ğŸŒŸ ä¼˜åŒ– B: ç»„åˆå¼å­¦ä¹ ç‡ç­–ç•¥
    warmup_scheduler = get_cosine_schedule_with_warmup(optimizer, 
                                                      num_warmup_steps=WARMUP_EPOCHS, 
                                                      num_training_steps=EPOCHS)
    # Plateau è°ƒåº¦å™¨ç›‘æ§éªŒè¯é›†å‡†ç¡®ç‡ï¼Œå¦‚æœåœ¨ 15 è½®å†…æ²¡æå‡ï¼Œåˆ™å‡åŠå­¦ä¹ ç‡
    plateau_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=15, verbose=True)
    
    ema = EMA(token_model, 0.999)
    ema.register()

    best_acc = 0.0
    patience_counter = 0

    for epoch in range(EPOCHS):
        token_model.train()
        vqvae.eval() # å§‹ç»ˆå†»ç»“ VQVAE
        
        all_preds, all_gts = [], []
        total_loss = 0

        for x, labels in train_loader:
            x, labels = x.to(device), labels.to(device)
            
            optimizer.zero_grad()
            with torch.no_grad():
                _, _, _, indices = vqvae(x)
                
                # ğŸŒŸ ä¼˜åŒ– C: è®­ç»ƒæ—¶å¢å¼º - éšæœºå¹³ç§» (Temporal Jitter)
                # è¿™èƒ½æœ‰æ•ˆé˜²æ­¢ Transformer è¿‡åº¦èƒŒè¯µ VQ åºåˆ—çš„ç»å¯¹ä½ç½®
                if torch.rand(1) > 0.5:
                    shift = torch.randint(-4, 5, (1,)).item()
                    indices = torch.roll(indices, shifts=shift, dims=1)

            logits = token_model(indices)
            loss = criterion(logits, labels)
            loss.backward()
            
            # ğŸŒŸ ä¼˜åŒ– D: ä¸¥æ ¼æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢ ECoG å¼‚å¸¸æ³¢åŠ¨
            torch.nn.utils.clip_grad_norm_(token_model.parameters(), max_norm=0.5)
            optimizer.step()
            ema.update()

            _, predicted = torch.max(logits, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_gts.extend(labels.cpu().numpy())
            total_loss += loss.item()

        # è°ƒæ•´å­¦ä¹ ç‡
        if epoch < WARMUP_EPOCHS:
            warmup_scheduler.step()

        # ğŸŒŸ éªŒè¯é˜¶æ®µï¼šåˆ‡æ¢åˆ° EMA æƒé‡è¿›è¡Œå…¬å¹³æµ‹è¯•
        ema.apply_shadow()
        val_acc = test(epoch)
        
        # æ›´æ–° Plateau è°ƒåº¦å™¨
        if epoch >= WARMUP_EPOCHS:
            plateau_scheduler.step(val_acc)
        
        # æ‰“å°è¯¦ç»†æŠ¥å‘Š
        train_acc = np.mean(np.array(all_preds) == np.array(all_gts)) * 100
        pred_dist = Counter(all_preds)
        print(f"\n--- Epoch {epoch} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc*100:.2f}% | LR: {optimizer.param_groups[0]['lr']:.8f}")
        print(f"é¢„æµ‹åˆ†å¸ƒ: {[f'C{k}: {v/len(all_preds)*100:.1f}%' for k, v in sorted(pred_dist.items())]}")

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            patience_counter = 0
            torch.save(token_model.state_dict(), tokenmodel_path.replace(".pth", "_best.pth"))
            print(f"â­ New Best Model Saved (Acc: {best_acc*100:.2f}%)")
        else:
            patience_counter += 1

        ema.restore() # æ¢å¤åŸå§‹å‚æ•°ç»§ç»­ä¸‹ä¸€è½®è®­ç»ƒ

        # ğŸŒŸ ä¼˜åŒ– E: æ—©åœæœºåˆ¶ (Early Stopping)
        if patience_counter > 60: # å¦‚æœ 60 è½®æ²¡æå‡åˆ™åœæ­¢
            print("è§¦å‘æ—©åœï¼Œè®­ç»ƒç»“æŸã€‚")
            break
        
# --- 4. æµ‹è¯•è„šæœ¬ ---
from test_VQTokenizer import diagnose_vq_collapse, calculate_accuracy, save_confusion_matrix, analyze_token_distribution
def test(epoch):
    token_model.eval()
    vqvae.eval()
    all_preds, all_gts = [], []
    
    with torch.no_grad():
        for x, labels in test_loader:
            x, labels = x.to(device), labels.to(device)
            _, _, _, indices = vqvae(x)
            logits = token_model(indices)
            _, preds = torch.max(logits, 1)
            all_preds.extend(preds.cpu().numpy())
            all_gts.extend(labels.cpu().numpy())

    final_acc = np.mean(np.array(all_preds) == np.array(all_gts))
    save_confusion_matrix(all_gts, all_preds, epoch, final_acc, LABEL_MAP, save_path)
    
    print(f"\nâœ… éªŒè¯å®Œæˆ! å¹³å‡å‡†ç¡®ç‡: {final_acc*100:.2f}%")
    return final_acc


def test0(epoch):
    print(">>> å¼€å§‹æµ‹è¯• TransformerVQToTokenModel å‰å‘ä¼ æ’­...")
    # å½¢çŠ¶ä¸º [Batch, Seq_Len]ï¼Œå€¼åœ¨ 0 åˆ° CODEBOOK_SIZE-1 ä¹‹é—´
    vqvae.eval()
    token_model.eval()

    total_acc = 0
    total_samples = 0
    # criterion = nn.CrossEntropyLoss()
    # # å»ºç«‹ä¸€ä¸ªåå‘æ˜ å°„ï¼Œç”¨äºæ‰“å°ç»“æœ (ID -> æ–‡æœ¬)
    # id_to_text = {v: aligner.tokenizer.decode([v]) for v in aligner.gt_token_map.values()}
    all_preds = []
    all_gts = []
    with torch.no_grad():            
        for x, labels in test_loader:
            x, labels = x.to(device), labels.to(device)
            # 1. æå– VQ ç´¢å¼•
            x_recon, z_e, z_q, indices = vqvae(x)  # print("VQVAE Indices shape: ", indices.shape)
            # 2. è·å– GT Tokens
            if 'vocab6' in save_path:
                gt_tokens = aligner.get_local_labels(labels) # [B]
            else:
                gt_tokens = aligner.get_gt_batch(labels) # [B]
            # print("GT Tokens shape: ", gt_tokens.shape) torch.Size([1])
            
            # 3. å‰å‘ä¼ æ’­ # 4. è®¡ç®—æŸå¤±
            logits = token_model(indices)
            # print(f"è¾“å…¥å½¢çŠ¶: {indices.shape}") # [1, 250]
            # print(f"è¾“å‡ºå½¢çŠ¶ (Logits): {logits.shape}, GT {gt_tokens.shape}") # [1, 151643]
            # è¾“å‡ºå½¢çŠ¶ (Logits): torch.Size([1, 6])
                
            # loss = criterion(logits, gt_tokens)
            acc, preds = calculate_accuracy(logits, gt_tokens)
            
            # æ”¶é›†æ•°æ®ç”¨äºæ··æ·†çŸ©é˜µ
            all_preds.extend(preds.cpu().numpy())
            all_gts.extend(gt_tokens.cpu().numpy())
            
            total_acc += acc * x.size(0)
            total_samples += x.size(0) 
            # print(f"Loss: {loss.item():.4f} | Batch Acc: {acc*100:.2f}%")
            # # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
            # for i in range(min(20000000, x.size(0))):
            #     pred_text = aligner.tokenizer.decode([preds[i]])
            #     gt_text = aligner.tokenizer.decode([gt_tokens[i]])
            #     print(f"  Sample {i}: Predicted: '{pred_text}'({preds[i]}), GT: '{gt_text}'({gt_tokens[i]})")

    final_acc = total_acc / total_samples # 1822
    print(f"\nâœ… éªŒè¯å®Œæˆ! æ€»æµ‹è¯•æ ·æœ¬: {total_samples}, å¹³å‡å‡†ç¡®ç‡: {final_acc*100:.2f}%")
    # --- æ ¸å¿ƒé€»è¾‘ï¼šè®¡ç®—å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ ---
    save_confusion_matrix(all_gts, all_preds, epoch, final_acc, LABEL_MAP, save_path)
    
    print(f"\nâœ… éªŒè¯å®Œæˆ! å¹³å‡å‡†ç¡®ç‡: {final_acc*100:.2f}%")
    return final_acc


if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    BATCH_SIZE = 42  # 
    EPOCHS = 1000
    LEARNING_RATE = 1e-4
    codebook = 512  # 128  # 256  #
    VQVAEembed_dim = 64  # 1  # 
    VQToTokenembed_dim = 256  # 128  # 64  # 
    subset = 'fingerflex'  # 'motor_basic'  # 'gestures' # 'BCIComp4'  #  'joystick_track2' #
    winsize = 1000  # 1024  # 
    WARMUP_EPOCHS = 10  # å‰ 10 ä¸ª epoch æ…¢æ…¢å¢åŠ  LR
    
    if subset =='BCIComp4': 
        save_path = './checkpoints/BCI Competition IV/TokenAlign_c%d_emb%d/' % (codebook, VQToTokenembed_dim)  # 
        VQVAEmodelpath = "./checkpoints/BCI Competition IV/VQTokenizer/best_vqvae_model.pth"
        
        datadir = '/mnt/home/user1/MCX/EEGLM/data/ECoG/BCICIV_4_mat/'
        full_dataset = BCIComp4Dataset4_single(datadir=datadir, window_size=winsize, 
                stride=256, single_channel=True, channel_idx=None) # å•é€šé“stride=512,
        # full_dataset = BCIComp4Dataset4(datadir=datadir, window_size=winsize, stride=256)
    else:
        save_path = "./checkpoints/Stanford/TokenAlign/%s_vocab6-2/" % (subset)
        # save_path = "./checkpoints/Stanford/TokenAlign/%s_c%d/" % (subset, codebook)
        
        VQVAEmodelpath = "./checkpoints/Stanford/VQTokenizer/%s_c%d_2/vqvae_model.pth" % (subset, codebook) # best_
        # random.seed(0)
        datadir = '/disk2/user1/dataset/BCI-Standford/%s/%s/data/' % (subset, subset)
        full_dataset = MillerFingersDataset(datadir, window_size=winsize, stride=256,
                        single_channel=True, channel_idx=None, subset=subset)  # å•é€šé“ channel_idx=0,

    num_classes = full_dataset.num_classes
    
    aligner = TokenAlignmentTrainer(subset=subset)    
    vqvae = NeuralVQVAE(in_channels=1, codebook_size=codebook, 
                        embed_dim=VQVAEembed_dim, num_classes=num_classes).to(device)  # 
    
    vqvae.load_state_dict(torch.load(VQVAEmodelpath)['model_state_dict'])
    
    # åˆå§‹åŒ–å¯¹é½æ¨¡å‹
    os.makedirs(save_path, exist_ok=True)
    tokenmodel_path = f"{save_path}token_aligner.pth"
    # token_model = TransformerVQToTokenModel(codebook_size=codebook, embed_dim=VQToTokenembed_dim,
    #         nhead=8, num_layers=4, num_classes=len(LABEL_MAP), vocab_size=len(aligner.tokenizer), 
    #         max_seq_len=250).to(device)
    
    with torch.no_grad():
        vqvae.eval()
        test_input = torch.zeros(1, 1, winsize).to(device)
        _, _, _, dummy_indices = vqvae(test_input)
        actual_seq_len = dummy_indices.shape[1]
    print(f"Detected VQ Sequence Length: {actual_seq_len}")
    token_model = TransformerVQToTokenModel6_Robust(
        codebook_size=codebook, 
        embed_dim=VQToTokenembed_dim,
        num_classes=len(LABEL_MAP), 
        max_seq_len=actual_seq_len 
    ).to(device)
    
    print("...åŠ è½½ ECoG æ•°æ®é›†...")
    # torch.manual_seed(0)
    train_loader, test_loader, test_data = prepare_dataloaders(full_dataset, BATCH_SIZE, num_classes=len(LABEL_MAP), ratio=0.9)
    # train_data, test_data = random_split(full_dataset, [int(len(full_dataset)*0.9),
    #     len(full_dataset)-int(len(full_dataset)*0.9)])    
    # print(f"æ•°æ®é›†åŠ è½½æˆåŠŸ: è®­ç»ƒæ ·æœ¬æ•°={len(train_data)}, æµ‹è¯•æ ·æœ¬æ•°={len(test_data)}")
    # sampler = ClassBalancedBatchSampler(train_data, batch_size=BATCH_SIZE, num_classes=6, 
    #                         samples_per_class=BATCH_SIZE//6)
    # train_loader = DataLoader(train_data, batch_sampler=sampler)
    # # train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
    # test_loader = DataLoader(test_data, batch_size=1, shuffle=False)
    
    # analyze_token_distribution(vqvae, train_loader, test_loader, codebook, device)
    # exit()
    
    # diagnose_vq_collapse(test_data, vqvae, device, len(full_dataset)-int(len(full_dataset)*0.9))
    # # diagnose_vq_collapse(full_dataset, vqvae, device)

    train_alignment()
    
    # æ‰§è¡Œæµ‹è¯•
    print("åŠ è½½å·²æœ‰çš„ Token å¯¹é½æ¨¡å‹æƒé‡...")
    token_model.load_state_dict(torch.load(tokenmodel_path))
    test(EPOCHS)
    token_model.load_state_dict(torch.load(tokenmodel_path.replace('.pth', '_best.pth')))
    test(0)
    
    