import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt
from data import MillerFingersDataset, ClassBalancedBatchSampler, prepare_dataloaders
from test_VQTokenizer import plot_reconstruction_comparison, diagnose_vq_collapse, analyze_token_distribution
import os

# ==========================================
# 1. å¿«é€Ÿæ›´æ–°çš„ VQ å±‚
# ==========================================
class VectorQuantizerEMA(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25, decay=0.9, epsilon=1e-5):
        super().__init__()
        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings
        self._commitment_cost = commitment_cost
        
        self._embedding = nn.Embedding(num_embeddings, embedding_dim)
        self._embedding.weight.data.normal_()
        
        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))
        self.register_buffer('_ema_w', torch.Tensor(num_embeddings, embedding_dim))
        self._ema_w.data.copy_(self._embedding.weight.data)
        
        self._decay = decay
        self._epsilon = epsilon

    def forward(self, inputs):
        input_shape = inputs.shape
        flat_input = inputs.permute(0, 2, 1).reshape(-1, self._embedding_dim)

        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) 
                    + torch.sum(self._embedding.weight**2, dim=1)
                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))

        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.size(0), self._num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)

        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape[0], input_shape[2], self._embedding_dim).permute(0, 2, 1)
        
        if self.training:
            with torch.no_grad():
                self._ema_cluster_size.mul_(self._decay).add_(torch.sum(encodings, dim=0), alpha=1 - self._decay)
                n = torch.sum(self._ema_cluster_size)
                self._ema_cluster_size.copy_(
                    (self._ema_cluster_size + self._epsilon) / (n + self._num_embeddings * self._epsilon) * n)
                dw = torch.matmul(encodings.t(), flat_input)
                self._ema_w.mul_(self._decay).add_(dw, alpha=1 - self._decay)
                self._embedding.weight.data.copy_(self._ema_w / self._ema_cluster_size.unsqueeze(1))

        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        loss = self._commitment_cost * e_latent_loss
        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))

        return loss, quantized, perplexity, encoding_indices.view(input_shape[0], input_shape[2])

# ==========================================
# 2. å¼ºåŒ–ç‰ˆ Encoder (åŠ å…¥åŸå‹å¹³æ»‘)
# ==========================================
class NeuralVQVAE(nn.Module):
    def __init__(self, in_channels=1, codebook_size=512, embed_dim=64, num_classes=6):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv1d(in_channels, embed_dim, kernel_size=4, stride=2, padding=1),
            nn.InstanceNorm1d(embed_dim),
            nn.ReLU(),
            nn.Conv1d(embed_dim, embed_dim, kernel_size=4, stride=2, padding=1),
            nn.InstanceNorm1d(embed_dim),
            nn.ReLU(),
            # âœ… æ–°å¢ AvgPoolï¼šä¸‹é‡‡æ ·æŠ›å¼ƒé«˜é¢‘å™ªå£°ï¼Œå¼ºåˆ¶æå–ç¼“æ…¢å˜åŒ–çš„è¯­ä¹‰ç‰¹å¾
            nn.AvgPool1d(kernel_size=4, stride=4), 
            nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1),
        )

        # âœ… å†…ç½®ç±»åŸå‹ Buffer (ä¸å‚ä¸æ¢¯åº¦ä¸‹é™ï¼Œé€šè¿‡ EMA æ›´æ–°)
        self.register_buffer('class_prototypes', torch.zeros(num_classes, embed_dim))
        self.proto_decay = 0.99 

        self.aux_classifier = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(embed_dim, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes)
        )

        self.vq_layer = VectorQuantizerEMA(codebook_size, embed_dim, decay=0.9)

        self.decoder = nn.Sequential(
            # è¿™é‡Œçš„ä¸Šé‡‡æ ·éœ€åŒ¹é… Encoder çš„ä¸‹é‡‡æ ·å€æ•°
            nn.ConvTranspose1d(embed_dim, embed_dim, kernel_size=16, stride=16), 
            nn.ReLU(),
            nn.ConvTranspose1d(embed_dim, in_channels, kernel_size=1, stride=1),
        )

    def forward(self, x):
        z_e = self.encoder(x)
        cls_logits = self.aux_classifier(z_e)
        vq_loss, z_q, perplexity, indices = self.vq_layer(z_e)
        x_recon = self.decoder(z_q)
        
        # è¡¥å…¨é‡æ„å°ºå¯¸ (ç”±äº Pooling å¯èƒ½å¯¼è‡´é•¿åº¦å¾®å·®)
        if x_recon.shape[-1] != x.shape[-1]:
            x_recon = F.interpolate(x_recon, size=x.shape[-1], mode='linear', align_corners=False)

        if self.training:
            return vq_loss, x_recon, indices, cls_logits, perplexity, z_e
        else:
            return x_recon, z_e, z_q, indices

    @torch.no_grad()
    def update_prototypes(self, z_v, labels):
        """ ä½¿ç”¨å½“å‰ Batch æ›´æ–°å…¨å±€ç±»ä¸­å¿ƒ """
        for l in range(self.class_prototypes.size(0)):
            mask = (labels == l)
            if mask.any():
                batch_mean = z_v[mask].mean(0)
                # EMA æ›´æ–°åŸå‹
                self.class_prototypes[l] = self.proto_decay * self.class_prototypes[l] + \
                                          (1 - self.proto_decay) * batch_mean

# ==========================================
# 3. åŠŸèƒ½å®Œæ•´çš„ Main
# ==========================================
def main():
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    # ä½¿ç”¨ ReduceLROnPlateauï¼Œå½“é‡æ„æŸå¤±è¿›å…¥å¹³å°æœŸæ—¶é™ä½å­¦ä¹ ç‡
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=12, factor=0.5)
    
    start_epoch = 0
    modelpath = os.path.join(save_path, "vqvae_model.pth")
    bestmodelpath = modelpath.replace("vqvae_model.pth", "best_vqvae_model.pth")
    best_val_loss = float('inf')
    # if os.path.exists(bestmodelpath):
    #     checkpoint = torch.load(bestmodelpath, map_location=device)
    #     model.load_state_dict(checkpoint['model_state_dict'])
    #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    #     start_epoch = checkpoint['epoch'] + 1
    #     best_val_loss = checkpoint['loss']
    #     print(f'æ¢å¤è®­ç»ƒï¼šEpoch {start_epoch}, Best Loss: {best_val_loss:.4f}')

    # åˆå§‹è¶…å‚æ•°
    aux_weight = 20.0     # åˆ†ç±»å¤´æƒé‡
    sim_weight = 50.0     # ç±»é—´æ’æ–¥æƒé‡
    # intra_weight = 0.0  # ğŸš€ å¼ºåŒ–ï¼šç±»å†…å‡èšæƒé‡ (è§£å†³ç›¸ä¼¼åº¦ä½çš„æ ¸å¿ƒ)
    # 1. Intra-Dist ä¼šå›å‡ï¼ˆæ¯”å¦‚åˆ° 0.2ï¼‰ï¼Œè¿™æ˜¯å¥½äº‹ï¼è¯´æ˜ç‰¹å¾ç©ºé—´ä¸å†æ˜¯æ­»çš„ã€‚
    # 2. Inter-Sim ä¼šå¼€å§‹ä¸‹é™ï¼ˆç›®æ ‡æ˜¯ç ´æ‰ 0.90ï¼‰ï¼Œè¿™ä»£è¡¨ç±»é—´åŒºåˆ†åº¦å‡ºæ¥äº†ã€‚
    sample_sim_weight = 30.0 # æ ·æœ¬çº§æƒé‡
    recon_weight = 0.2     # æä½æƒé‡ï¼Œåªç»™ Encoder ç•™ä¸€å£æ°”
    
    print("\nğŸš€ å¼€å§‹å¼ºåŒ–ç‰ˆå…¨ç±»èšåˆ VQ-Tokenizer è®­ç»ƒ...")    
    for epoch in range(start_epoch, EPOCHS):
        model.train()
        total_recon = 0
        total_aux = 0
        # total_intra = 0
        # total_inter = 0        
        for x, labels in train_loader:
            x, labels = x.to(device), labels.to(device)
            x = (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + 1e-6)

            optimizer.zero_grad()
            # 1. Forward
            vq_loss, x_recon, indices, cls_logits, _, z_e = model(x)            
            # 2. åŸºç¡€æŸå¤±
            recon_loss = F.mse_loss(x_recon, x)
            aux_loss = F.cross_entropy(cls_logits, labels)
            # 3. æ ¸å¿ƒï¼šæ ·æœ¬çº§å¯¹æ¯”æŸå¤± (Sample-level Contrastive Loss)
            # z_v: [Batch, Channel]
            z_v = z_e.mean(dim=-1)             
            # ğŸ”¥ å…³é”®æ­¥éª¤ï¼šç‰¹å¾å½’ä¸€åŒ–åˆ°å•ä½çƒï¼Œæ¶ˆé™¤å¹…å€¼å¹²æ‰°ï¼Œå¼ºåˆ¶å…³æ³¨æ–¹å‘å·®å¼‚
            z_v_norm = F.normalize(z_v, p=2, dim=1)            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ [Batch, Batch]
            sim_matrix = torch.matmul(z_v_norm, z_v_norm.t())            
            # æ„é€ æ©ç ï¼šåªæœ‰æ ‡ç­¾ä¸åŒçš„æ ·æœ¬å¯¹æ‰ä¸º True
            diff_label_mask = (labels.unsqueeze(0) != labels.unsqueeze(1)).float()
            # ğŸŒŸ æ ·æœ¬çº§æ’æ–¥ï¼šå¦‚æœä¸åŒç±»æ ·æœ¬ç›¸ä¼¼åº¦ > 0.3ï¼Œå°±æ–½åŠ æƒ©ç½š
            # ä½¿ç”¨è¾ƒé«˜çš„ Margin (0.3) å¼ºåˆ¶æ‹‰å¼€è·ç¦»
            sample_inter_penalty = (torch.clamp(sim_matrix - 0.3, min=0) * diff_label_mask).sum() / (diff_label_mask.sum() + 1e-6)
            
            # 4. åŸå‹çº§æ’æ–¥ (ä¿æŒåŸæœ‰çš„å…¨å±€çº¦æŸ)
            model.update_prototypes(z_v.detach(), labels)
            all_protos = F.normalize(model.class_prototypes, p=2, dim=1)
            proto_sims = torch.matmul(all_protos, all_protos.t())
            mask = torch.eye(num_classes, device=device).bool()
            proto_inter_penalty = torch.clamp(proto_sims[~mask] - 0.4, min=0).pow(2).mean()

            # 5. ç»„åˆæ€»æŸå¤±
            # âš ï¸ æ³¨æ„ï¼šåˆæœŸæˆ‘ä»¬å°† recon_weight è®¾å¾—å¾ˆä½ï¼Œsample_sim_weight è®¾å¾—å¾ˆé«˜
            loss = (recon_weight * recon_loss) + vq_loss + (aux_weight * aux_loss) + \
                   (sim_weight * proto_inter_penalty) + (sample_sim_weight * sample_inter_penalty)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_recon += recon_loss.item()
            total_aux += aux_loss.item()

        # 4. éªŒè¯ä¸æŒ‡æ ‡è¯Šæ–­
        model.eval()
        val_recon_list = []
        # 1. è®¡ç®—æµ‹è¯•é›†çš„ Inter-Sim (ç±»é—´ä¸­å¿ƒç›¸ä¼¼åº¦)
        current_inter_sim = get_inter_class_similarity(model, test_data, device, num_samples=len(full_dataset)-int(len(full_dataset)*0.9))
        # 2. è®¡ç®—æµ‹è¯•é›†çš„å¹³å‡ Intra-Dist (ç±»å†…è·ç¦» 1-Sim)
        total_val_intra_dist = 0
        samples_count = 0
        with torch.no_grad():
            for vx, vl in test_loader:
                vx, vl = vx.to(device), vl.to(device)
                v_recon, z_e, _, _ = model(vx)
                
                # è®¡ç®—é‡æ„ Loss
                val_recon_list.append(F.mse_loss(v_recon, vx).item())
                
                # è®¡ç®—è¯¥æ ·æœ¬åˆ°å…¶ç±»ä¸­å¿ƒçš„è·ç¦»
                z_v = z_e.mean(dim=-1)
                target_proto = model.class_prototypes[vl]
                cos_sim = F.cosine_similarity(z_v, target_proto)
                total_val_intra_dist += (1.0 - cos_sim).mean().item()
                samples_count += 1

        avg_val_recon = sum(val_recon_list) / len(val_recon_list)
        avg_val_intra = total_val_intra_dist / samples_count
        # --- 3. è‡ªåŠ¨åŒ–çŠ¶æ€åˆ¤å®šé€»è¾‘ ---
        if current_inter_sim > 0.95 and avg_val_intra < 0.05:
            status = "ğŸ”´ ç‰¹å¾åå¡Œ (Collapse) - ç±»é—´åˆ†ä¸å¼€"
        elif current_inter_sim > 0.85:
            status = "ğŸŸ¡ æ­£åœ¨è§£è€¦ (Breakout) - å°è¯•æ¨å¼€è¾¹ç•Œ"
        elif current_inter_sim <= 0.85 and avg_val_intra > 0.1:
            status = "ğŸŸ¢ ç†æƒ³çŠ¶æ€ (Golden) - ç±»é—´ç–ç¦»ä¸”ç±»å†…æœ‰ç»†èŠ‚"
        else:
            status = "ğŸ”µ è°ƒä¼˜ä¸­ (Fine-tuning)"
        # --- 4. æ ¼å¼åŒ–è¾“å‡º ---
        print("-" * 70)
        print(status)
        if current_inter_sim > 0.8:
            sim_weight = min(sim_weight + 5.0, 100.0)
            aux_weight = min(aux_weight + 2.0, 50.0)
            sample_sim_weight = min(sample_sim_weight + 5.0, 100.0) # ğŸŒŸ ä¹Ÿè¦åŠ¨æ€åŠ å‹æ ·æœ¬çº§æŸå¤±
            recon_weight = max(recon_weight - 0.05, 0.1) # ğŸŒŸ å…è®¸æ›´ä½çš„é‡æ„æƒé‡ï¼Œå¼ºåˆ¶è§£è€¦
        else:
            recon_weight = min(recon_weight + 0.1, 1.0) # ğŸŒŸ è§£è€¦åæ…¢æ…¢æ¢å¤é‡æ„
            sample_sim_weight = max(sample_sim_weight - 2.0, 10.0) # ğŸŒŸ è§£è€¦åé™ä½æ’æ–¥å‹åŠ›
        # if avg_val_intra < 0.05:
        #     intra_weight = 0.0
        # else:
        #     intra_weight = 1.0
        avg_val_loss = avg_val_recon
        scheduler.step(avg_val_loss)
        
        # 5. æ¨¡å‹ä¿å­˜é€»è¾‘
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_val_loss,
            }, bestmodelpath)
            print(f"â­ New Best Model! Val Recon: {avg_val_loss:.4f}")

        # 6. æ‰“å°è¿›åº¦æŠ¥å‘Š  Intra-Dist è¶Šå°ä»£è¡¨ç±»å†…è¶Šèšåˆï¼›Inter-Sim è¶Šå°ä»£è¡¨ç±»é—´è¶ŠåŒºåˆ†
        print(f"Epoch {epoch:03d} | Recon: {total_recon/len(train_loader):.4f} | "
              f"In Valid: \nIntra-Dist: {avg_val_intra:.4f} | Inter-Sim: {current_inter_sim:.4f} | "
              f"LR: {optimizer.param_groups[0]['lr']:.2e}")
        print(f"âš™ï¸ å½“å‰æƒé‡æ‰§è¡Œ: Recon_w: {recon_weight:.1f} | Sim_w: {sim_weight:.1f}")

        # 7. å®šæœŸè¯Šæ–­
        if (epoch + 1) % 20 == 0:
            diagnose_vq_collapse(test_data, model, device, num_samples=len(full_dataset)-int(len(full_dataset)*0.9))
            
        # 8. çƒ­é‡å¯æœºåˆ¶ (é˜²æ­¢å­¦ä¹ ç‡è¿‡ä½å¯¼è‡´æ­»é”)
        if optimizer.param_groups[0]['lr'] < 1e-6:
            for param_group in optimizer.param_groups:
                param_group['lr'] = LEARNING_RATE
            optimizer.state.clear() 
            print("ğŸš€ Learning rate reset!")

        # ä¿å­˜ Last Checkpoint
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_val_loss,
        }, modelpath)

    print("\nè®­ç»ƒæµç¨‹ç»“æŸã€‚")
    

# å¦‚æœå®ƒå¼€å§‹ä¸‹é™åˆ° 0.8-0.9 å·¦å³ï¼Œä½ çš„ VQVAE å°±ç®—çœŸæ­£è®­ç»ƒæˆåŠŸäº†ï¼Œæ­¤æ—¶å†è®­ç»ƒä¹‹å‰çš„ Transformer å°±ä¼šæœ‰ç«‹ç«¿è§å½±çš„æ•ˆæœã€‚
def get_inter_class_similarity(model, dataset, device, num_samples=100):
    model.eval()
    vectors = {0: [], 1: []}
    with torch.no_grad(): # âœ… å¼ºåˆ¶ä¸è¿½è¸ªæ¢¯åº¦
        for x, label in dataset:
            l = label.item()
            if l in vectors and len(vectors[l]) < num_samples:
                x = x.unsqueeze(0).to(device)
                z_e = model.encoder(x)
                # ä½¿ç”¨ .detach() ç¡®ä¿å½»åº•æ–­å¼€è®¡ç®—å›¾
                vectors[l].append(z_e.view(-1).detach()) 
            if len(vectors[0]) >= num_samples and len(vectors[1]) >= num_samples:
                break
    
    if len(vectors[0]) < 2 or len(vectors[1]) < 2: return 1.0
    
    mu0 = torch.stack(vectors[0]).mean(0)
    mu1 = torch.stack(vectors[1]).mean(0)
    # è¿”å›çº¯ Python æµ®ç‚¹æ•°
    sim = F.cosine_similarity(mu0.unsqueeze(0), mu1.unsqueeze(0)).item()
    return sim

 
def test(save_path0, isbest=True, channel_idx=0):
    test_loader = DataLoader(test_data, batch_size=1, shuffle=False) #BATCH_SIZE
    print(f"æ•°æ®é›†åŠ è½½æˆåŠŸ: æµ‹è¯•æ ·æœ¬æ•°={len(test_data)}")
    modelpath = os.path.join(save_path, "vqvae_model.pth")
    bestmodelpath = modelpath.replace("vqvae_model.pth", "best_vqvae_model.pth")
    save_path1 = save_path0 + '/last/'   #
    if isbest:
        modelpath = bestmodelpath
        save_path1 = save_path0 + '/best/'  # 
    checkpoint = torch.load(modelpath)
    os.makedirs(save_path1, exist_ok=True)
    model.load_state_dict(checkpoint['model_state_dict'])
    print('Finetune from', modelpath)
    model.eval()
    
    diagnose_vq_collapse(test_data, model, device, len(full_dataset)-int(len(full_dataset)*0.9))
        
    # 6. æ‰§è¡Œæœ€ç»ˆè¯„ä¼°
    print("\n--- æ‰§è¡Œå®Œæ•´éªŒè¯ Pipeline ---")
    print("--- 1. Waveform Fidelity Check ---")
    test_batch, test_labels = next(iter(test_loader))
    test_batch = test_batch.to(device)
    all_idx = []
    
    model.eval()
    with torch.no_grad():
        x_recon, z_e, z_q, all_indices= model(test_batch)
        all_idx.append(all_indices.view(-1).cpu())
    # ç”»å‡ºç¬¬ 0 ä¸ªæ ·æœ¬çš„ç¬¬ 5 å·é€šé“é‡æ„å›¾
    plot_reconstruction_comparison(test_batch, x_recon, channel_idx=channel_idx, savepath=save_path1)
    
    print("\n--- 3. Codebook Utilization Check ---")
    # ç»Ÿè®¡ Codebook çš„æ¿€æ´»æ¯”ä¾‹
    unique_tokens = torch.unique(all_indices).cpu().numpy()
    print(f"Active Tokens in this batch: {len(unique_tokens)} / {model.vq_layer._num_embeddings}")

    if len(unique_tokens) < 10:
        print("âš ï¸ Warning: Codebook Collapse detected! Use EMA or Dead Code Reset.")
    else:
        print("âœ… Codebook utilization is healthy.")
        
    analyze_token_distribution(model, train_loader, test_loader, codebook, device)

    # ç»Ÿè®¡æµ‹è¯•é›†é‡Œæ‰€æœ‰ indices çš„åˆ†å¸ƒ  
    all_idx = torch.cat(all_idx)
    plt.hist(all_idx.numpy(), bins=512)
    plt.title("Codebook Usage Distribution")
    plt.savefig(save_path1+'Codebook_Usage_Distribution.png', dpi=300, bbox_inches='tight')
    plt.show()


if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    BATCH_SIZE = 42  # 64  #
    EPOCHS = 1000
    LEARNING_RATE = 1e-4
    codebook = 512  # 256  # 128  #
    winsize = 1024
    
    subset = 'fingerflex'  # 'joystick_track'  # 'gestures'  # 'motor_basic'  #
    save_path = "./checkpoints/Stanford/VQTokenizer/%s_c%d_3/" % (subset, codebook) #  
    # save_path = "./checkpoints/Stanford/VQTokenizer/%s_c%d_2/" % (subset, codebook) #  
    os.makedirs(save_path, exist_ok=True)
    
    print("æ­£åœ¨åŠ è½½ ECoG æ•°æ®é›†...")
    datadir = '/disk2/user1/dataset/BCI-Standford/%s/%s/data/' % (subset, subset)
    full_dataset = MillerFingersDataset(datadir, window_size=winsize, stride=256,
                        single_channel=True, channel_idx=None, subset=subset)
    num_classes = full_dataset.num_classes
    
    # æ—¶åºåˆ‡åˆ†
    train_loader, test_loader, test_data = prepare_dataloaders(full_dataset, BATCH_SIZE, num_classes=num_classes, ratio=0.9)
    # torch.manual_seed(0)  
    # train_data, test_data = random_split(full_dataset, [int(len(full_dataset)*0.9),
    #     len(full_dataset)-int(len(full_dataset)*0.9)])
    # sampler = ClassBalancedBatchSampler(train_data, batch_size=BATCH_SIZE, num_classes=6, 
    #                         samples_per_class=BATCH_SIZE//6)
    # train_loader = DataLoader(train_data, batch_sampler=sampler)
    # # train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
    # test_loader = DataLoader(test_data, batch_size=1, shuffle=False)
    
    # 4. æ¨¡å‹åˆå§‹åŒ– (in_channels éœ€åŒ¹é…æ•°æ®é›†é€šé“æ•°ï¼ŒMiller æ•°æ®é€šå¸¸æ˜¯ 64)
    model = NeuralVQVAE(in_channels=1, codebook_size=codebook, 
                        embed_dim=64, num_classes=num_classes).to(device)
    
    main()
    
    print("\n--- æ‰§è¡Œå®Œæ•´éªŒè¯ Pipeline ---")
    test(save_path, isbest=True)
    test(save_path, isbest=False)
    
    
