import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, confusion_matrix
import seaborn as sns
import os
from torch.utils.data import DataLoader, random_split
from data import MillerFingersDataset, BCIComp4Dataset4, \
    BCIComp4TestDataset, BCIComp4Dataset4_single

# ==========================================
# 1. ç‰©ç†ç»´åº¦ï¼šæ³¢å½¢é‡æ„å¯¹æ¯”å›¾ (Reconstruction Visualization)
# ==========================================
def plot_reconstruction_comparison(original, reconstructed, channel_idx=0, 
                                   num_samples=1, savepath='./'):
    """
    å¯¹æ¯”åŸå§‹ ECoG æ³¢å½¢ä¸é‡æ„åçš„æ³¢å½¢
    original/reconstructed: [B, C, T]
    """
    plt.figure(figsize=(12, 4))
    orig = original[0, channel_idx, :].cpu().numpy()
    recon = reconstructed[0, channel_idx, :].cpu().numpy()
    
    time = np.arange(len(orig))
    plt.plot(time, orig, label='Original ECoG', color='blue', alpha=0.6)
    plt.plot(time, recon, label='Reconstructed (from Tokens)', color='red', linestyle='--')
    
    # è®¡ç®—ç›¸å…³ç³»æ•° (Pearson Correlation)
    correlation = np.corrcoef(orig, recon)[0, 1]
    
    plt.title(f"Channel {channel_idx} Reconstruction (Corr: {correlation:.3f})")
    plt.legend()
    plt.xlabel("Time Samples")
    plt.ylabel("Normalized Amplitude")
    plt.grid(True, alpha=0.3)
    plt.savefig(savepath+"rec_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()


# ==========================================
# 2. è¯­ä¹‰ç»´åº¦ï¼št-SNE èšç±»åˆ†æ (Semantic Clustering)
# ==========================================
def run_tsne_analysis(model, test_loader, device, savepath):
    """
    éªŒè¯ Token æ˜¯å¦åœ¨éšç©ºé—´å†…å½¢æˆäº†å…·æœ‰è¯­ä¹‰çš„ç¥ç»åŸè¯­ç°‡
    """
    model.eval()
    all_features = []
    all_labels = []
    
    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(device)
            # æå–é‡åŒ–åçš„ç‰¹å¾ç´¢å¼•ï¼Œå°†å…¶å±•å¹³ä½œä¸ºç‰¹å¾å‘é‡
            _, _, indices = model(x) # indices: [B, T_compressed, 1]
            feat = indices.view(x.size(0), -1).cpu().numpy()
            all_features.append(feat)
            all_labels.append(y.view(-1).numpy())
            
    all_features = np.concatenate(all_features, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)
    
    # æ‰§è¡Œ t-SNE
    print("Running t-SNE... (this may take a minute)")
    tsne = TSNE(n_components=2, perplexity=30, init='pca', learning_rate='auto')
    embeds = tsne.fit_transform(all_features)
    
    # è®¡ç®—è½®å»“ç³»æ•° (Silhouette Score) - è¯æ˜èšç±»è´¨é‡çš„é‡åŒ–æŒ‡æ ‡
    ss = silhouette_score(embeds, all_labels)
    
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(embeds[:, 0], embeds[:, 1], c=all_labels, cmap='Set1', s=20, alpha=0.8)
    plt.colorbar(scatter, label='Action Label (Finger Index)')
    plt.title(f"t-SNE of Neural Tokens (Silhouette Score: {ss:.3f})")
    plt.savefig(savepath+"t-SNE_Neural_Tokens.png", dpi=300, bbox_inches='tight')
    plt.show()
    return ss


def run_tsne_analysis_raw(test_loader, savepath):
    all_features = []
    all_labels = []
    
    with torch.no_grad():
        for x, y in test_loader:
            feat = x.reshape(x.shape[0], -1) # x.view(x.size(0), -1)
            all_features.append(feat)
            all_labels.append(y.view(-1).numpy())
            
    all_features = np.concatenate(all_features, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)
    
    # æ‰§è¡Œ t-SNE
    print("Running t-SNE... (this may take a minute)")
    tsne = TSNE(n_components=2, perplexity=30, init='pca', learning_rate='auto')
    embeds = tsne.fit_transform(all_features)
    
    # è®¡ç®—è½®å»“ç³»æ•° (Silhouette Score) - è¯æ˜èšç±»è´¨é‡çš„é‡åŒ–æŒ‡æ ‡
    ss = silhouette_score(embeds, all_labels)
    
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(embeds[:, 0], embeds[:, 1], c=all_labels, cmap='Set1', s=20, alpha=0.8)
    plt.colorbar(scatter, label='Action Label (Finger Index)')
    plt.title(f"t-SNE of Neural Tokens (Silhouette Score: {ss:.3f})")
    plt.savefig(savepath+"raw_t-SNE_Neural_Tokens.png", dpi=300, bbox_inches='tight')
    plt.show()
    return ss


# ==========================================
# 3. æ€§èƒ½ç»´åº¦ï¼šåˆ†ç±»å‡†ç¡®ç‡ä¸æ··æ·†çŸ©é˜µ (Linear Probing)
# ==========================================
def evaluate_classification_performance(model, test_loader, device, savepath):
    """
    ä½¿ç”¨ç®€å•çš„çº¿æ€§å±‚æˆ–ç®€å•çš„ MLP éªŒè¯ Token çš„ä¿¡æ¯ä¸°å¯Œåº¦
    å¦‚æœ Token é€‰å¾—å¥½ï¼Œå³ä½¿ä¸çœ‹åŸå§‹æ³¢å½¢ï¼Œåˆ†ç±»å‡†ç¡®ç‡ä¹Ÿåº”è¯¥å¾ˆé«˜
    """
    model.eval()
    preds, targets = [], []
    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(device)
            # è·å– Token åºåˆ—
            _, _, indices = model(x)
            # ç®€æ˜“è§£ç å™¨é€»è¾‘ï¼šè¿™é‡Œæˆ‘ä»¬ç›´æ¥ç”¨ Token çš„åˆ†å¸ƒç‰¹å¾åšç»Ÿè®¡
            # åœ¨å®é™…è®ºæ–‡ä¸­ï¼Œè¿™é‡Œä¼šæ¥ä¸€ä¸ªå¾®å°çš„ Linear Layer
            # æ­¤æ—¶ä¸ºäº†å¿«é€ŸéªŒè¯ï¼Œæˆ‘ä»¬è®¡ç®—æ¯ä¸ª Batch çš„é¢„æµ‹è¶‹åŠ¿
            feat = indices.view(x.size(0), -1) 
            # æ¨¡æ‹Ÿï¼šç®€å•ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬æœ€å¸¸å‡ºç°çš„ Tokenï¼ˆä»…ä½œå±•ç¤ºï¼‰
            # å»ºè®®ï¼šå®é™…ç§‘ç ”ä¸­ï¼Œè®­ç»ƒä¸€ä¸ª Linear Probe SVM æ›´å¥½
            
    # è¾“å‡ºæ··æ·†çŸ©é˜µ (Confusion Matrix)
    # æ­¤å¤„å‡è®¾ä½ å·²ç»è®­ç»ƒäº†ä¸€ä¸ªç®€æ˜“çš„ Linear Probe åˆ†ç±»å™¨
    cm = confusion_matrix(targets, preds)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d')# , cmap='Blues'
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.savefig(savepath+'confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()

# ==========================================
# 4. æ‰§è¡Œå®Œæ•´éªŒè¯ Pipeline
# ==========================================
def run_full_validation(model, test_loader, device, savepath, channel_idx=5):
    print("--- 1. Waveform Fidelity Check ---")
    # å–ä¸€ä¸ª Batch è¿›è¡Œå¯è§†åŒ–
    test_batch, test_labels = next(iter(test_loader))
    test_batch = test_batch.to(device)
    
    model.eval()
    with torch.no_grad():
        _, x_recon, _ = model(test_batch)
    
    # ç”»å‡ºç¬¬ 0 ä¸ªæ ·æœ¬çš„ç¬¬ 5 å·é€šé“é‡æ„å›¾
    plot_reconstruction_comparison(test_batch, x_recon, channel_idx=channel_idx, savepath=savepath)
    
    print("\n--- 2. Semantic Clustering Check ---")
    silhouette = run_tsne_analysis(model, test_loader, device, savepath)
    
    print("\n--- 3. Codebook Utilization Check ---")
    # ç»Ÿè®¡ Codebook çš„æ¿€æ´»æ¯”ä¾‹
    _, _, all_indices = model(test_batch)
    unique_tokens = torch.unique(all_indices).cpu().numpy()
    print(f"Active Tokens in this batch: {len(unique_tokens)} / {model.vq_layer._num_embeddings}")
    
    if len(unique_tokens) < 10:
        print("âš ï¸ Warning: Codebook Collapse detected! Use EMA or Dead Code Reset.")
    else:
        print("âœ… Codebook utilization is healthy.")


def visualize_tsne(model, dataloader, device, savepath):
    """æå– t-SNE çš„è¾…åŠ©å‡½æ•°"""
    model.eval()
    all_indices = []
    all_labels = []    
    with torch.no_grad():
        for x, y in dataloader:
            x = x.to(device)
            _, _, indices = model(x)
            feat = indices.view(x.size(0), -1).cpu().numpy()
            all_indices.append(feat)
            all_labels.append(y.numpy())
            
    all_indices = np.concatenate(all_indices, axis=0)
    all_labels = np.concatenate(all_labels, axis=0).flatten()
    
    tsne = TSNE(n_components=2, perplexity=30)
    X_embedded = tsne.fit_transform(all_indices)

    plt.figure(figsize=(10, 7))
    scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=all_labels, cmap='tab10', alpha=0.7)
    plt.colorbar(scatter, label='Finger Index')
    plt.title("Neural Primitive Clusters on Test Set")
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    plt.savefig(savepath+"tsne_clusters.png", dpi=300, bbox_inches='tight')
    plt.show()
    

def visualize_tsne_raw(dataloader, savepath):
    """æå– t-SNE çš„è¾…åŠ©å‡½æ•°"""
    all_indices = []
    all_labels = []    
    with torch.no_grad():
        for x, y in dataloader:
            feat = x.view(x.size(0), -1)
            all_indices.append(feat)
            all_labels.append(y.numpy())
            
    all_indices = np.concatenate(all_indices, axis=0)
    all_labels = np.concatenate(all_labels, axis=0).flatten()
    
    tsne = TSNE(n_components=2, perplexity=30)
    X_embedded = tsne.fit_transform(all_indices)

    plt.figure(figsize=(10, 7))
    scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=all_labels, cmap='tab10', alpha=0.7)
    plt.colorbar(scatter, label='Finger Index')
    plt.title("Neural Primitive Clusters on Test Set")
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    plt.savefig(savepath+"rawdata_tsne_clusters.png", dpi=300, bbox_inches='tight')
    plt.show()
 

def runRawdata():
    savepath = './checkpoints/rawdata_analysis/'
    os.makedirs(savepath, exist_ok=True)
    
    full_dataset = BCIComp4Dataset4_single(datadir = '/mnt/home/user1/MCX/EEGLM/data/ECoG/BCICIV_4_mat/',
                    window_size=1000, stride=512, single_channel=True # å•é€šé“
                               , channel_idx=None)

    train_data, test_data = random_split(full_dataset, [int(len(full_dataset)*0.9),
        len(full_dataset)-int(len(full_dataset)*0.9)])
    
    test_loader = DataLoader(test_data, batch_size=1, shuffle=False)
    print("\n--- ç”Ÿæˆæœ€ç»ˆ t-SNE å¯è§†åŒ– ---")# t-SNE å›¾ï¼š5ä¸ªæ‰‹æŒ‡çš„èšç±»é¢œè‰²åŒºåˆ†æ˜æ˜¾
    # æ­¤å¤„å¯ä»¥ä½¿ç”¨ test_loader æ¥è¯„ä¼°æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„èšç±»æ•ˆæœ
    visualize_tsne_raw(test_loader, savepath)
    run_tsne_analysis_raw(test_loader, savepath)
    

# ======================================================
# VQ ç©ºé—´æ·±åº¦è¯Šæ–­æŠ¥å‘Š
# ======================================================
import torch.nn.functional as F
def diagnose_vq_collapse(dataset, vqvae, device, num_samples=20):
    vqvae.eval()
    # å­˜å‚¨åŸå§‹ç‰¹å¾ (Encoder Output)
    ze_0, ze_1 = [], []
    # å­˜å‚¨é‡åŒ–ç‰¹å¾ (Quantized Output)
    zq_0, zq_1 = [], []
    class0_vectors = []
    class1_vectors = []
    
    print(f"\n" + "="*50)
    print(f"ğŸ” VQ ç©ºé—´æ·±åº¦è¯Šæ–­æŠ¥å‘Š (æ ·æœ¬æ•°: {num_samples})")
    print("="*50)
    with torch.no_grad():
        found0, found1 = 0, 0
        for x, label in dataset:
            x = x.unsqueeze(0).to(device)
            # è·å–æ‰€æœ‰ä¸­é—´å˜é‡
            x_recon, z_e, z_q, indices = vqvae(x)
            # å±•å¹³ä¸º [T * embed_dim] æ–¹ä¾¿è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            vec_q = z_q.view(-1) 
            vec_e = z_e.view(-1)
            
            if label == 0 and found0 < num_samples:
                class0_vectors.append(vec_q)
                ze_0.append(vec_e)
                zq_0.append(vec_q)
                found0 += 1
            elif label == 1 and found1 < num_samples:
                class1_vectors.append(vec_q)
                ze_1.append(vec_e)
                zq_1.append(vec_q)
                found1 += 1
            if found0 >= num_samples and found1 >= num_samples: break
                                                
    # è®¡ç®—å‡å€¼å‘é‡
    m_ze0, m_ze1 = torch.stack(ze_0).mean(0), torch.stack(ze_1).mean(0)
    m_zq0, m_zq1 = torch.stack(zq_0).mean(0), torch.stack(zq_1).mean(0)
    # --- è®¡ç®—ç›¸ä¼¼åº¦ ---
    # 1. Encoder é˜¶æ®µç›¸ä¼¼åº¦ (åˆ¤æ–­ Encoder æ˜¯å¦æœ‰åŒºåˆ†èƒ½åŠ›)
    sim_ze = F.cosine_similarity(m_ze0.unsqueeze(0), m_ze1.unsqueeze(0)).item()
    # 2. VQ é˜¶æ®µç›¸ä¼¼åº¦ (åˆ¤æ–­ Codebook æ˜¯å¦åå¡Œ)
    sim_zq = F.cosine_similarity(m_zq0.unsqueeze(0), m_zq1.unsqueeze(0)).item()
    print(f"1. Encoder åŸå§‹ç‰¹å¾ç›¸ä¼¼åº¦ (z_e): {sim_ze:.4f}")
    print(f"2. Codebook é‡åŒ–ç‰¹å¾ç›¸ä¼¼åº¦ (z_q): {sim_zq:.4f}")
    print("-" * 30)
    # --- é€»è¾‘åˆ¤å®š ---
    if sim_ze > 0.98:
        print("âŒ åˆ¤å®šç»“æœ: [Encoder æ­»äº¡]")
        print("åŸå› : Encoder æ ¹æœ¬æ²¡æœ‰å°è¯•å»åŒºåˆ†ä¸åŒçš„åŠ¨ä½œã€‚")
        print("å¯¹ç­–: å¢åŠ  Aux Loss æƒé‡ï¼Œæˆ–å¼•å…¥ Triplet Lossï¼Œæ£€æŸ¥æ•°æ®æ˜¯å¦å½’ä¸€åŒ–ã€‚")
    elif sim_zq > 0.98 and sim_ze < 0.90:
        print("âŒ åˆ¤å®šç»“æœ: [VQ/Codebook åå¡Œ]")
        print("åŸå› : Encoder åˆ†å¼€äº†ç‰¹å¾ï¼Œä½†æ‰€æœ‰ç‰¹å¾éƒ½è¢«æ˜ å°„åˆ°äº†åŒä¸€ä¸ªæˆ–æå…¶ç›¸ä¼¼çš„ Code ä¸Šã€‚")
        print("å¯¹ç­–: é™ä½ EMA Decay (è‡³ 0.8)ï¼Œå¢åŠ  Codebook å­¦ä¹ ç‡ï¼Œæˆ–å¼€å¯æ­»ç é‡å¯ã€‚")
    elif sim_zq < 0.90:
        print("âœ… åˆ¤å®šç»“æœ: [æ¨¡å‹å¥åº·]")
        print("ç‰¹å¾åœ¨ç¼–ç å’Œé‡åŒ–é˜¶æ®µéƒ½ä¿æŒäº†åŒºåˆ†åº¦ã€‚")
    else:
        print("âš ï¸ åˆ¤å®šç»“æœ: [è¾¹ç¼˜çŠ¶æ€]")
        print("æœ‰ä¸€å®šçš„åŒºåˆ†åº¦ï¼Œä½†å»ºè®®ç»§ç»­åŠ å¤§è¾…åŠ©åˆ†ç±»å‹åŠ›ã€‚")
    print("="*50 + "\n")

    # 2. è®¡ç®—ç›¸ä¼¼åº¦
    c0_mean = torch.stack(class0_vectors).mean(0)
    c1_mean = torch.stack(class1_vectors).mean(0)
    # ç±»å†…ç›¸ä¼¼åº¦ (Intra-class)
    # sim_00 = torch.nn.functional.cosine_similarity(class0_vectors[0], class0_vectors[1], dim=0)
    sim_00 = compute_intra_sim(ze_0)
    # ç±»é—´ç›¸ä¼¼åº¦ (Inter-class)
    sim_01 = torch.nn.functional.cosine_similarity(c0_mean, c1_mean, dim=0)
    print(f"ç±»0 å†…éƒ¨ç›¸ä¼¼åº¦: {sim_00:.4f} (è¶Šæ¥è¿‘ 1 è¯´æ˜ç¼–ç è¶Šç¨³å®š)")
    print(f"ç±»0 ä¸ ç±»1 çš„ç±»é—´ç›¸ä¼¼åº¦: {sim_01.item():.4f}")
    if sim_01 > 0.95:
        print("âš ï¸ è­¦å‘Š: ç±»é—´ç›¸ä¼¼åº¦æé«˜ï¼VQVAE æ— æ³•åŒºåˆ†é™æ¯å’ŒåŠ¨ä½œï¼ŒTransformer å¾ˆéš¾å­¦ã€‚")
    else:
        print("âœ… ç¼–ç å…·æœ‰åŒºåˆ†åº¦ï¼Œè¯·é‡ç‚¹æ£€æŸ¥åˆ†ç±»å¤´çš„ Loss æƒé‡ã€‚")

def compute_intra_sim(vectors):
    v_stack = torch.stack(vectors) # [N, D]
    v_norm = F.normalize(v_stack, p=2, dim=1)
    sim_matrix = torch.matmul(v_norm, v_norm.t()) # [N, N]
    # å–ä¸Šä¸‰è§’ï¼ˆä¸å«å¯¹è§’çº¿ï¼‰çš„å¹³å‡å€¼
    mask = torch.triu(torch.ones_like(sim_matrix), diagonal=1).bool()
    return sim_matrix[mask].mean().item()


def calculate_accuracy(logits, targets):
    """
    è®¡ç®— Top-1 å‡†ç¡®ç‡
    logits: [Batch, Vocab_Size]
    targets: [Batch] (Qwen Token IDs)
    """
    assert logits.device == targets.device, "Tensor on different devices"

    # è·å–é¢„æµ‹çš„ Token ID (æ¦‚ç‡æœ€å¤§çš„ç´¢å¼•)
    _, predicted = torch.max(logits, dim=1)

    # å¯¹æ¯”é¢„æµ‹å€¼ä¸çœŸå€¼
    correct = (predicted == targets).sum().item()
    total = targets.size(0)

    accuracy = correct / total
    return accuracy, predicted


def save_confusion_matrix(y_true, y_pred, epoch, acc, LABEL_MAP, save_path):
    """ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µå›¾ç‰‡"""
    # è·å–ç±»åˆ«åç§°
    classes = [LABEL_MAP[i] for i in range(len(LABEL_MAP))]
    
    # è®¡ç®—çŸ©é˜µ (å½’ä¸€åŒ–åˆ° 0-1 èŒƒå›´ï¼Œçœ‹æ¯”ä¾‹æ›´ç›´è§‚)
    cm = confusion_matrix(y_true, y_pred, labels=range(len(classes)))
    cm_perc = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-9) # é˜²æ­¢é™¤0
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_perc, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=classes, yticklabels=classes)
    
    plt.title(f'Confusion Matrix (Epoch {epoch} | Acc: {acc*100:.2f}%)')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    
    # ä¿å­˜å›¾ç‰‡
    img_path = os.path.join(save_path, f"cm_epoch_{epoch}.png")
    plt.savefig(img_path)
    plt.close() # é‡Šæ”¾å†…å­˜
    print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {img_path}")

from collections import Counter
def analyze_token_distribution(vqvae, train_loader, test_loader, codebook_size, device):
    vqvae.eval()
    train_tokens = []
    test_tokens = []

    print("æ­£åœ¨æå–è®­ç»ƒé›† Token...")
    with torch.no_grad():
        for x, _ in train_loader:
            _, _, _, indices = vqvae(x.to(device))
            train_tokens.extend(indices.view(-1).cpu().numpy())

    print("æ­£åœ¨æå–éªŒè¯é›† Token...")
    with torch.no_grad():
        for x, _ in test_loader:
            _, _, _, indices = vqvae(x.to(device))
            test_tokens.extend(indices.view(-1).cpu().numpy())

    # ç»Ÿè®¡é¢‘ç‡
    train_counts = Counter(train_tokens)
    test_counts = Counter(test_tokens)

    # è½¬æ¢ä¸ºå¯†åº¦åˆ†å¸ƒï¼ˆå½’ä¸€åŒ–ï¼‰
    train_dist = np.array([train_counts.get(i, 0) for i in range(codebook_size)])
    test_dist = np.array([test_counts.get(i, 0) for i in range(codebook_size)])
    train_dist = train_dist / (train_dist.sum() + 1e-9)
    test_dist = test_dist / (test_dist.sum() + 1e-9)

    # å¯è§†åŒ–
    plt.figure(figsize=(15, 6))
    plt.bar(range(codebook_size), train_dist, alpha=0.5, label='Train Tokens', color='blue')
    plt.bar(range(codebook_size), test_dist, alpha=0.5, label='Test Tokens', color='red')
    plt.title("Token Activation Distribution: Train vs Test")
    plt.xlabel("Token ID")
    plt.ylabel("Activation Density")
    plt.legend()
    plt.savefig("token_dist_comparison.png")
    plt.show()

    # è®¡ç®—é‡åˆåº¦ (Intersection over Union)
    active_train = set(np.where(train_dist > 0)[0])
    active_test = set(np.where(test_dist > 0)[0])
    overlap = active_train.intersection(active_test)
    
    print(f"\n--- è¯Šæ–­æŠ¥å‘Š ---")
    print(f"è®­ç»ƒé›†æ¿€æ´» Token æ•°: {len(active_train)}")
    print(f"éªŒè¯é›†æ¿€æ´» Token æ•°: {len(active_test)}")
    print(f"ä¸¤è€…é‡åˆ Token æ•°: {len(overlap)}")
    if len(active_test) > 0:
        print(f"éªŒè¯é›† Token è¦†ç›–ç‡: {len(overlap)/len(active_test)*100:.2f}%")
        
 
 
# runRawdata()

# åœ¨è®­ç»ƒè„šæœ¬æœ«å°¾è°ƒç”¨ï¼š
# run_full_validation(model, test_loader, device)

'''
ç›¸å…³ç³»æ•° (Correlation)ï¼š
å¯¹äº ECoG ä¿¡å·ï¼Œå¦‚æœç›¸å…³ç³»æ•° $r > 0.7$ï¼Œè¯´æ˜ VQ ç¼–ç å™¨æˆåŠŸä¿ç•™
å¤§éƒ¨åˆ†ç‰©ç†ç‰¹å¾ã€‚åœ¨ 1000:1 çš„å‹ç¼©æ¯”ä¸‹åšåˆ°è¿™ä¸€ç‚¹ã€‚
t-SNE èšç±»ï¼š
å¤±è´¥çš„è¡¨ç°ï¼š æ‰€æœ‰é¢œè‰²çš„ç‚¹æ··åœ¨ä¸€èµ·ã€‚
æˆåŠŸçš„è¡¨ç°ï¼š åŒä¸€ç§é¢œè‰²ï¼ˆåŒä¸€ä¸ªåŠ¨ä½œï¼‰çš„ç‚¹å½¢æˆäº†ä¸€ä¸ªæ˜ç¡®çš„å­¤å²›ã€‚
è¯æ˜ä½ å®šä¹‰çš„â€œç¥ç»åŸè¯­â€åœ¨æ•°å­¦ä¸Šæ˜¯å¯åˆ†çš„ã€‚
Codebook åˆ©ç”¨ç‡ï¼š
å¦‚æœ 512 ä¸ª Token ä¸­æœ‰ 50-200 ä¸ªè¢«é¢‘ç¹æ¿€æ´»ï¼Œ
è¯´æ˜æ¨¡å‹å­¦åˆ°è„‘ç”µä¿¡å·ä¸­ä¸°å¯Œçš„äºšç¨³æ€ã€‚
'''